<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
  <title>Notes on MLE for SciPy continuous distributions</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script>
    window.MathJax = {
        tex: {
            tags: 'ams'
        }
    };
  </script>
  <style>
  body {
    margin: 42px;
  }
  </style>
</head>
<body>
<ul>
  <li><a href="#powerlaw">Power law distribution</a></li>
  <li><a href="#rayleigh">Rayleigh distribution</a></li>
</ul>

<h2 id="powerlaw">Power law distribution</h2>
<p>
Parameters
<ul>
<li>\(a\): shape, \(a > 0\)</li>
<li>\(\mu\): location</li>
<li>\(\sigma\): scale, \(\sigma\) > 0</li>
</ul>
<p>
Support: \(\mu < x \le \mu + \sigma\)
<p>
PDF:
\[
  f(a, \mu, \sigma) = \frac{a}{\sigma}\left(\frac{x - \mu}{\sigma}\right)^{a-1}
  = \frac{a}{\sigma^a}\left(x - \mu\right)^{a-1}
\]
log PDF:
\[
  \log f(a, \mu, \sigma) = \log a - a\log\sigma + (a - 1)\log(x - \mu)
\]
The likelihood function for the vector \(\textbf{x} = \{x_1, x_2, \ldots, x_N\}\):
\[
  L(\textbf{x}, a, \mu, \sigma)
    = \prod_{i=1}^{N} \frac{a}{\sigma^a}\left(x_i - \mu\right)^{a-1}
    = \frac{a^{N}}{\sigma^{Na}}\prod_{i=1}^{N} \left(x_i - \mu\right)^{a-1}
\]
The log-likelihood function:
\[
  \ell(\textbf{x}, a, \mu, \sigma) = N\log a - N a \log\sigma + (a - 1)\sum_{i=1}^{N}\log(x_i - \mu)
\]
</p>
<h4>Maximum likelihood estimation</h4>
<p>
The power law distribution has some technical issues that might require
that we impose additional constraints on the inputs to the MLE fit procedure.
</p>
<p>
<dl>
<dt>\(\mu\)</dt>
<dd>
<p>
Note that as \(\mu\) varies, the only term
in \(\ell(\textbf{x}, a, \mu, \sigma)\) that changes is
\((a - 1)\sum_{i=1}^{N}\log(x_i - \mu)\).
</p>
<p>
If \(a > 1\), decreasing \(\mu\) causes that term to increase, so for the MLE, make \(\mu\)
as small as possible.  For each \(x_{i}\) to be in the support of the
distribution, we require \(x_{\max} \le \mu + \sigma\), or \(\mu \ge x_{\max} - \sigma\), we have the constraint
\[
  \mu = x_{\max} - \sigma
\]
</p>
<p>
If \(0 < a < 1\), the sign of that term changes, so for the MLE,
we want to make \(\mu\) as large as possible.  This leads to
\[
  \mu = x_{\min}
\]
</p>
</dd>
<dt>\(\sigma\)</dt>
<dd>
<p>
Note that as \(\sigma\) varies, the only term
in \(\ell(\textbf{x}, a, \mu, \sigma)\) that changes is
\(-N a \log \sigma\).
To maximize this term, make \(\sigma\) as small as possible.
To ensure that each \(x_{i}\) is in the support of the distribution, the
smallest that \(\sigma\) can be is \(x_{\max} - x_{\min}\), but the
general constraint is that
\[
  \sigma = x_{\max} - \mu
\]
</p>
</dd>
<dt>\(a\)</dt>
<dd>
<p>
We can find the MLE for \(a\) through the first order condition for the extremum
of \(\ell(\textbf{x}, a, \mu, \sigma)\).  We have
\[
  \frac{\partial \ell}{\partial a} =
    \frac{N}{a} + \sum_{i=1}^{N} \log\left(\frac{x_i - \mu}{\sigma}\right)
\]
By setting \(\frac{\partial \ell}{\partial a} = 0\), we obtain
\[
  a = \frac{-N}{\sum_{i=1}^{N} \log\left(\frac{x_i - \mu}{\sigma}\right)}
\]
</p>
</dd>
</dl>
</p>
<p>
Suppose we know a priori that \(a > 1\).
<ul>
<li>If both \(\mu\) and \(\sigma\) are free,
the above shows that the MLE solution must satisfy the constraint
\[
  \sigma + \mu - x_{\max} = 0
\]
</li>
<li>If \(\mu\) is fixed and \(\sigma\) is free, the MLE for \(\sigma\)
is
\[
  \sigma = x_{\max} - \mu
\]
</li>
<li>If \(\sigma\) is fixed and \(\mu\) is free, the MLE for \(\mu\)
is
\[
  \mu = x_{\max} - \sigma
\]
</li>
</ul>
</p>
<p>
If we know a priori that \(0 < a < 1\), a problem arises.
The discussion of \(\sigma\) still applies: if \(\sigma > x_{\max} - \mu\)
(and \(\mu < x_{\min}\)), we can increase the likelihood by decreasing
\(\sigma\).  So, for a fixed \(\mu < x_{\min}\), the MLE
for \(\sigma\) is  \(\sigma = x_{\max} - \mu\).
</p>
<p>
The earlier discussion of \(\mu\) above shows that, if \(\mu < x_{\min}\),
we can increase the likelihood by increasing \(\mu\).  That suggests
that the MLE for \(\mu\) is \(\mu = x_{\min}\).
</p>
<p>
So it looks like the MLE for \(\mu\) and \(\sigma\) is
\[
    \mu = x_{\min}
\]
\[
    \sigma = x_{\max} - x_{\min}
\]
The problem with this solution is that setting \(\mu = x_{\min}\) means
that the term \((x_i - \mu)\) in the likelihood function is zero when
\(x_i\) is \(x_{\min}\).  Then \((x_i - \mu)^{a - 1}\) "blows up" (i.e.
the PDF blows up at \(x = x_{\min})\).
Stated more carefully, the likelihood function diverges to infinity as
\(\mu\) approaches \(x_{\min}\).  Should we then just accept this
as the MLE?  The problem then is that it leaves \(a\) undetermined,
because the blow up happens for any \(a\) in the interval
\(0 < a < 1\).

</p>
<h2 id="rayleigh">Rayleigh distribution</h2>
<p>
Parameters
<ul>
<li>\(\mu\): location</li>
<li>\(\sigma\): scale</li>
</ul>
</p>
<p>
The PDF:
\[f(x, \mu, \sigma) = \frac{1}{\sigma}\left(\frac{x-\mu}{\sigma}\right)
                      \exp\left(-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^{2}\right)
\]
The likelihood function for the vector \(\textbf{x} = \{x_1, x_2, \ldots, x_N\} \):
\[
L(\textbf{x}, \mu, \sigma) = \prod_{i=1}^{N}\frac{1}{\sigma}\left(\frac{x_i-\mu}{\sigma}\right)
                      \exp\left(-\frac{1}{2}\left(\frac{x_i - \mu}{\sigma}\right)^{2}\right)
\]
The log-likelihood function:
\[
\begin{split}
\ell(\textbf{x}, \mu, \sigma)
 & = \sum_{i=1}^{N} \left[
    -2\log\sigma
    + \log(x_i-\mu)
    - \frac{1}{2}\left(\frac{x_i - \mu}{\sigma}\right)^{2}
    \right] \\
 & =
    -2N\log\sigma
    + \sum_{i=1}^{N} \left[
        \log(x_i-\mu)
        - \frac{1}{2}\left(\frac{x_i - \mu}{\sigma}\right)^{2}
      \right]
\end{split}
\]
Equations for the critical points:
\begin{equation}
\frac{\partial \ell}{\partial \sigma} =
   \frac{-2N}{\sigma} + \frac{1}{\sigma^3}\sum_{i=1}^{N} (x_i - \mu)^2
\end{equation}
Setting \(\frac{\partial \ell}{\partial \sigma} = 0\) gives
\begin{equation}
    \sigma^2 = \frac{1}{2N}\sum_{i=1}^{N} (x_i - \mu)^2
\end{equation}
If the location parameter \(\mu\) is fixed, we're done.
If \(\mu\) is not fixed, we need \(\frac{\partial \ell}{\partial \mu} \)
\begin{equation}
\frac{\partial \ell}{\partial \mu} =
    \sum_{i=1}^{N} \left[ \frac{-1}{x_i - \mu} + \frac{x_i - \mu}{\sigma^2} \right]
\end{equation}
Setting \(\frac{\partial \ell}{\partial \mu} = 0\) gives
\begin{equation}
  \sum_{i=1}^{N}(x_i - \mu) - \sigma^2 \sum_{i=1}^{N}\frac{1}{x_i - \mu} = 0
\end{equation}
With neither \(\mu\) nor \(\sigma\) fixed, we have two equations to solve
simultaneously.  There isn't an explicit solution, but we can use the
expression for \(\sigma^2\) to reduce the problem to a single equation
for \(\mu\) that must be solved numerically:
\begin{equation}
  \sum_{i=1}^{N}(x_i - \mu) - \left(\frac{1}{2N}\sum_{i=1}^{N} (x_i - \mu)^2\right) \sum_{i=1}^{N}\frac{1}{x_i - \mu} = 0

\end{equation}
</p>
</body>
</html>
